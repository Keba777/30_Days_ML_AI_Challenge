# Day 14: Intro to Deep Learning

## What I Learned
- **Deep Learning Basics**:
  - What Deep Learning (DL) is and how it differs from traditional machine learning.
  - The role of activation functions (e.g., sigmoid, ReLU) in neural networks.
  - Common optimization techniques such as Gradient Descent, Stochastic Gradient Descent (SGD), and Adam.
  
- **Neural Networks from Scratch**:
  - Building a simple neural network using only NumPy.
  - Implementing forward propagation and backpropagation.
  - Training a model on a toy dataset to understand weight updates.

## Tasks Completed
1. **Neural Network Implementation**:
   - Developed a simple feedforward neural network using NumPy.
   - Defined activation functions (sigmoid, ReLU) and their derivatives.
   - Implemented forward and backward propagation to train the network.
2. **Optimization**:
   - Applied basic gradient descent to update weights.
3. **Testing**:
   - Trained the network on a simple dataset (e.g., XOR or a synthetic classification problem) to demonstrate learning.

## Resources Used
- [Deep Learning Book by Ian Goodfellow](https://www.deeplearningbook.org/)
- [Neural Networks Demystified (YouTube Series)](https://www.youtube.com/watch?v=bxe2T-V8XRs)
- [NumPy Documentation](https://numpy.org/doc/)
- [Introduction to Neural Networks (Towards Data Science)](https://towardsdatascience.com/neural-network-from-scratch-in-python-using-numpy-f8b9424647b1)
