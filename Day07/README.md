# Day 07: Decision Trees & Random Forests

## What I Learned
- **Decision Trees**:
  - Understanding the structure of decision trees and their splitting criteria (Gini Impurity, Entropy).
  - Strengths and limitations of decision trees in classification tasks.
- **Random Forests**:
  - Concept of ensemble learning and how Random Forest improves over decision trees.
  - Key hyperparameters like the number of trees (`n_estimators`) and maximum tree depth.
- **Wine Dataset**:
  - Familiarity with Scikit-learn's Wine dataset and its features.

## Tasks Completed
1. **Decision Tree Classifier**:
   - Loaded and explored the Wine dataset.
   - Built and visualized a decision tree classifier.
2. **Random Forest Classifier**:
   - Built and trained a Random Forest model for wine classification.
   - Evaluated the model's performance using accuracy and classification report.
3. **Model Comparison**:
   - Compared the performance of the Decision Tree and Random Forest models.

## Resources Used
- [Scikit-learn Documentation for Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [Scikit-learn Documentation for Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [Understanding Decision Trees](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)
- [Introduction to Random Forests](https://machinelearningmastery.com/random-forest-algorithm-with-python/)
- [Wine Dataset Documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset)
